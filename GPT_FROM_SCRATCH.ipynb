{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPcRcYGpDRcKaqUOBsuhhPh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nkusi-Richard/COLAB-PROJECTS/blob/main/GPT_FROM_SCRATCH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "block_size = 64\n",
        "batch_size = 128\n",
        "\n",
        "max_iters = 10000\n",
        "eval_iters = 250\n",
        "n_embed = 384\n",
        "n_layer = 4\n",
        "dropout = 0.2\n",
        "n_head = 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahay0Kc6ACoE",
        "outputId": "167ffd68-1f3c-4e99-eeaf-052a756e85be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLCN1xYj0p5d",
        "outputId": "6e0389d5-919b-4d5e-dd74-eac7a3bcbfd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  DOROTHY AND THE WIZARD IN OZ\n",
            "\n",
            "  BY\n",
            "\n",
            "  L. FRANK BAUM\n",
            "\n",
            "  AUTHOR OF THE WIZARD OF OZ, THE LAND OF OZ, OZMA OF OZ, ETC.\n",
            "\n",
            "  ILLUSTRATED BY JOHN R. NEILL\n",
            "\n",
            "  BOOKS OF WONDER WILLIAM MORROW & CO., INC. NEW \n",
            "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ],
      "source": [
        "with open ('/content/wizard_of_oz.txt', 'r', encoding= 'utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "print(text[:200])\n",
        "\n",
        "chars = sorted(set(text))\n",
        "print(chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder and Decoder. Encoder assigns int to chars, Decoder assings chars to int"
      ],
      "metadata": {
        "id": "SOJFlBny7huC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string_to_int = { char:i for i, char in enumerate(chars)}\n",
        "\n",
        "int_to_string = { i:char for i, char in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [string_to_int[c] for c in s]\n",
        "decode = lambda l: \"\".join([int_to_string[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)"
      ],
      "metadata": {
        "id": "_9J3qQhL7dtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the data"
      ],
      "metadata": {
        "id": "mZvA47nXCY9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training size is 80% of the whole data\n",
        "\n",
        "n = int(0.8*len(data))\n",
        "\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]"
      ],
      "metadata": {
        "id": "2nF3nfHiCV9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "some torch examples"
      ],
      "metadata": {
        "id": "2bzpFAACEZEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "randint = torch.randint(-100, 100, (6,))\n",
        "print(randint)\n",
        "\n",
        "tensor = torch.tensor([[1,2,3], [4,5,6]])\n",
        "print(tensor)\n",
        "\n",
        "zeros = torch.zeros((3,2))\n",
        "print(zeros)\n",
        "\n",
        "ones = torch.ones((3,2))\n",
        "print(ones)\n",
        "\n",
        "input = torch.empty(2,3)\n",
        "input\n",
        "\n",
        "arange = torch.arange(5)\n",
        "print(arange)\n",
        "\n",
        "linspace = torch.linspace(3,10, steps=5)\n",
        "linspace\n",
        "\n",
        "logspace = torch.logspace(start=-10, end=10, steps=5)\n",
        "logspace\n",
        "\n",
        "eye = torch.eye(5)\n",
        "print(eye)\n",
        "\n",
        "start_time = time.time()\n",
        "zeros = torch.zeros(2,2)\n",
        "end_time = time.time()\n",
        "\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "torch_rand_gpu = torch.rand(100, 100, 100, 100).to(device)\n",
        "torch_rand_gpu2 = torch.rand(100, 100, 100, 100).to(device)\n",
        "\n",
        "torch_rand_cpu = torch.rand(100, 100, 100, 100)\n",
        "torch_rand_cpu2 = torch.rand(100, 100, 100, 100)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "gpu_multiplication = torch_rand_gpu @ torch_rand_gpu2\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(end_time - start_time)\n",
        "\n",
        "start_time = time.time()\n",
        "cpu_multiplication = np.multiply(torch_rand_cpu , torch_rand_cpu2)\n",
        "\n",
        "end_time = time.time()\n",
        "print(end_time - start_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTci6f9sEXDV",
        "outputId": "6568f623-f586-4573-f823-d2dfee243f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-48,  12,  98, -61, -48,  79])\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.]])\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n",
            "tensor([0, 1, 2, 3, 4])\n",
            "tensor([[1., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 1.]])\n",
            "0.11809587478637695\n",
            "0.1564197540283203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probabilities = torch.tensor([0.5, 0.5])\n",
        "\n",
        "samples = torch.multinomial(probabilities, num_samples=1, replacement=True)\n",
        "\n",
        "print(samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lTafFchfSof",
        "outputId": "61d41066-e92a-47fd-e540-c7ad69f8e7bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.tensor([1, 2, 3, 4])\n",
        "out = torch.cat((tensor, torch.tensor([5])), dim=0)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsP5LX4pf66k",
        "outputId": "224228cb-20a3-42d8-9a8e-b638e1944ea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3, 4, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = torch.tril(torch.ones(5, 5))\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILMSQEx_hVUJ",
        "outputId": "7069843e-6018-4b43-b943-89ec2e1b69fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = torch.triu(torch.ones(5, 5))\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AYYvxekhnZA",
        "outputId": "ed1ece25-7782-4eb8-85b6-3fa2bba500b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1., 1., 1.],\n",
            "        [0., 1., 1., 1., 1.],\n",
            "        [0., 0., 1., 1., 1.],\n",
            "        [0., 0., 0., 1., 1.],\n",
            "        [0., 0., 0., 0., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = torch.zeros(5, 5).masked_fill(torch.tril(torch.ones(5,5)) == 0, float('-inf') )\n",
        "print(out)\n",
        "\n",
        "print(torch.exp(out))\n",
        "input = torch.zeros(2, 3, 4)\n",
        "print(input.transpose(0, 1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGZ40CoCh5Ee",
        "outputId": "a45166b6-d03f-456c-9cd4-240423ce1670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., -inf, -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., -inf, -inf],\n",
            "        [0., 0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0., 0.]])\n",
            "tensor([[1., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1.]])\n",
            "torch.Size([3, 2, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1 = torch.tensor([1, 2, 3])\n",
        "tensor2 = torch.tensor([1, 3, 2])\n",
        "tensor3 = torch.tensor([3, 2, 1])\n",
        "stack = torch.stack([tensor1, tensor2, tensor3])\n",
        "print(stack)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5dvXhHcmJ8x",
        "outputId": "d7b5ac7a-42c5-4347-ab7d-3c458e1acb97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [1, 3, 2],\n",
            "        [3, 2, 1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iDfvK4v_mJH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "input = torch.tensor([1, 2, 3, 4], dtype=torch.float)\n",
        "\n",
        "linear_layer = nn.Linear(4, 3, bias=False)\n",
        "print(linear_layer(input))\n",
        "\n",
        "print(torch.softmax(linear_layer(input), dim=0))\n",
        "\n",
        "\n",
        "print(F.softmax(linear_layer(input), dim=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYk5Eufo2Q8v",
        "outputId": "2aab7dd4-641e-4111-988b-17b4e44474b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.9212, 1.5074, 0.6583], grad_fn=<SqueezeBackward4>)\n",
            "tensor([0.5144, 0.3401, 0.1455], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([0.5144, 0.3401, 0.1455], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embeddings"
      ],
      "metadata": {
        "id": "US_yJf_Q86Jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(chars)\n",
        "embedding_dim = 6\n",
        "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
        "indices = torch.tensor([1, 2, 3, 4], dtype=torch.long)\n",
        "emb = embedding_layer(indices)\n",
        "print(emb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DGlBrTT85Tr",
        "outputId": "1086a5e9-646a-4534-892c-3b19dae9f3f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2.4692, -0.6631,  1.8234, -1.2775, -0.1305, -0.5238],\n",
            "        [-0.2195, -0.9336,  0.4795,  0.0438,  0.1355,  0.9338],\n",
            "        [-1.0994, -1.0881, -0.2712,  0.5439, -1.9703, -0.8633],\n",
            "        [ 1.8264, -0.6988, -1.4301, -0.6478,  2.2244, -0.4473]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get Batch"
      ],
      "metadata": {
        "id": "1_OsvXEoBdcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  idx = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "  x = torch.stack([data[i:i+block_size ] for i in idx])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in idx ])\n",
        "\n",
        "  x, y = x.to(device), y.to(device)\n",
        "\n",
        "  return x, y\n",
        "\n",
        "x, y = get_batch('train')\n",
        "\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "jER-ENrWBZ3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cec25eb0-6c91-46f9-ebdf-f55ecc04d19c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[69, 58, 68,  ...,  1, 76, 62],\n",
            "        [61, 58, 71,  ..., 72,  1, 55],\n",
            "        [62, 71,  1,  ...,  1, 55, 74],\n",
            "        ...,\n",
            "        [68, 58, 72,  ..., 23,  1, 56],\n",
            "        [72, 54, 62,  ...,  9,  1, 33],\n",
            "        [ 1, 69, 65,  ..., 58, 57,  1]], device='cuda:0')\n",
            "tensor([[58, 68, 69,  ..., 76, 62, 65],\n",
            "        [58, 71,  1,  ...,  1, 55, 58],\n",
            "        [71,  1, 61,  ..., 55, 74, 73],\n",
            "        ...,\n",
            "        [58, 72,  1,  ...,  1, 56, 54],\n",
            "        [54, 62, 57,  ...,  1, 33,  0],\n",
            "        [69, 65, 54,  ..., 57,  1, 65]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bigram Model"
      ],
      "metadata": {
        "id": "ebUrnaMTqha8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.look_up_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, index, targets=None):\n",
        "\n",
        "    logits = self.look_up_table(index)\n",
        "\n",
        "    if targets == None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, index, max_new_tokens):\n",
        "    for token in range(max_new_tokens):\n",
        "      logits, loss = self.forward(index)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "      next_index = torch.multinomial(probs, num_samples=1, replacement=True)\n",
        "\n",
        "      index = torch.cat((index, next_index), dim=-1)\n",
        "\n",
        "    return index\n",
        "\n",
        "model = BigramModel(len(chars))\n",
        "m = model.to(device)\n",
        "\n",
        "#index = torch.tensor([1, 2, 3, 4])\n",
        "\n",
        "#logits = model(index, targets=index)\n",
        "#print(logits.shape)"
      ],
      "metadata": {
        "id": "kZLjeipcqpgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estimating Losses"
      ],
      "metadata": {
        "id": "FE5ic0SqruxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "\n",
        "def estimates_losses():\n",
        "  splits = ['train', 'val']\n",
        "  out = {}\n",
        "  m.eval()\n",
        "  for split in splits:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      x, y = get_batch(split)\n",
        "      logits, loss = m.forward(x, y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "\n",
        "  m.train()\n",
        "  return out\n"
      ],
      "metadata": {
        "id": "e8BOpzGFrx-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop"
      ],
      "metadata": {
        "id": "xo24fxldgoC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=3e-4)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_iters == 0:\n",
        "    losses = estimates_losses()\n",
        "    print(losses)\n",
        "\n",
        "  x, y = get_batch('train')\n",
        "  logits, loss = m.forward(x, y)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())\n",
        "\n",
        "with open('model_001.pkl', 'wb') as f:\n",
        "  pickle.dump(m, f)\n",
        "print(\"model saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjlUiTBOgwpq",
        "outputId": "8711d3e5-ab06-4bf8-d4d5-233cd2cd1aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': tensor(8.6675), 'val': tensor(8.7149)}\n",
            "{'train': tensor(2.7803), 'val': tensor(2.8363)}\n",
            "{'train': tensor(2.5762), 'val': tensor(2.6295)}\n",
            "{'train': tensor(2.5183), 'val': tensor(2.5710)}\n",
            "{'train': tensor(2.4929), 'val': tensor(2.5484)}\n",
            "{'train': tensor(2.4818), 'val': tensor(2.5354)}\n",
            "{'train': tensor(2.4722), 'val': tensor(2.5258)}\n",
            "{'train': tensor(2.4667), 'val': tensor(2.5126)}\n",
            "{'train': tensor(2.4614), 'val': tensor(2.5073)}\n",
            "{'train': tensor(2.4496), 'val': tensor(2.5068)}\n",
            "{'train': tensor(2.4427), 'val': tensor(2.4969)}\n",
            "{'train': tensor(2.4320), 'val': tensor(2.4971)}\n",
            "{'train': tensor(2.4263), 'val': tensor(2.4907)}\n",
            "{'train': tensor(2.4211), 'val': tensor(2.4846)}\n",
            "{'train': tensor(2.4210), 'val': tensor(2.4873)}\n",
            "{'train': tensor(2.4208), 'val': tensor(2.4917)}\n",
            "{'train': tensor(2.4163), 'val': tensor(2.4894)}\n",
            "{'train': tensor(2.4164), 'val': tensor(2.4871)}\n",
            "{'train': tensor(2.4154), 'val': tensor(2.4845)}\n",
            "{'train': tensor(2.4175), 'val': tensor(2.4768)}\n",
            "{'train': tensor(2.4066), 'val': tensor(2.4786)}\n",
            "{'train': tensor(2.4068), 'val': tensor(2.4734)}\n",
            "{'train': tensor(2.4087), 'val': tensor(2.4771)}\n",
            "{'train': tensor(2.4068), 'val': tensor(2.4744)}\n",
            "{'train': tensor(2.4047), 'val': tensor(2.4773)}\n",
            "{'train': tensor(2.4024), 'val': tensor(2.4703)}\n",
            "{'train': tensor(2.4019), 'val': tensor(2.4775)}\n",
            "{'train': tensor(2.4013), 'val': tensor(2.4718)}\n",
            "{'train': tensor(2.3974), 'val': tensor(2.4708)}\n",
            "{'train': tensor(2.3998), 'val': tensor(2.4721)}\n",
            "{'train': tensor(2.3953), 'val': tensor(2.4712)}\n",
            "{'train': tensor(2.3966), 'val': tensor(2.4705)}\n",
            "{'train': tensor(2.3974), 'val': tensor(2.4723)}\n",
            "{'train': tensor(2.3962), 'val': tensor(2.4689)}\n",
            "{'train': tensor(2.3976), 'val': tensor(2.4712)}\n",
            "{'train': tensor(2.3942), 'val': tensor(2.4674)}\n",
            "{'train': tensor(2.3955), 'val': tensor(2.4680)}\n",
            "{'train': tensor(2.3963), 'val': tensor(2.4740)}\n",
            "{'train': tensor(2.3911), 'val': tensor(2.4677)}\n",
            "{'train': tensor(2.3920), 'val': tensor(2.4662)}\n",
            "2.393280506134033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(m.generate(context, max_new_tokens=12)[0].tolist())\n",
        "genearated_tokens = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(genearated_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gstIAqhE2VJ5",
        "outputId": "096d7274-ef1a-44e8-e289-99b1df48427a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 39, 45, 44, 61, 58, 54, 75, 62, 71, 72, 9]\n",
            "\n",
            "m wawasuse\n",
            "\n",
            "\n",
            "Oz walfedecinowast ly Theded t. cashey. hecay dellindery wawntreeenneengowirelo tierediemawnde wave Fiely de wavoinffave decaly Q\n",
            "Obedenly benssse hed,\"\n",
            "\n",
            "Oz tfowawawaveavored wa be b-hows. heceeders ts,\"\n",
            "OUThodeawave gl\n",
            "Oz ulffs pansqunowas,\"\n",
            "Oz Oz hy re\n",
            "Onodrwearawshere tushews,\"\n",
            "\n",
            "OUThangly. BL\n",
            "O1 dirowawa wely b,\"Yedelfave d wawb-he s,\"Yede So tirin sl\n",
            "m. chanoms t beche waranlino beelffly t trello thandisanom. wey we thedel. whawelfe\n",
            "Oz enow t_ce s,\"Thaino toowselfot ls,\"Tharom. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT: Decoder Only transformer"
      ],
      "metadata": {
        "id": "BSOiPvT6UDI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.look_up_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.positional_embedding = nn.Embedding(block_size, n_embed)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embed)\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.2)\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "  def forward(self, index, targets=None):\n",
        "\n",
        "    B, T = index.shape\n",
        "    logits = self.look_up_table(index)\n",
        "\n",
        "    pos = self.positional_embedding(torch.arange(T, device=device))\n",
        "    logits = logits + pos\n",
        "    logits = self.blocks(logits)\n",
        "    logits = self.ln_f(logits)\n",
        "    logits = self.lm_head(logits)\n",
        "    if targets == None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, index, max_new_tokens):\n",
        "    for token in range(max_new_tokens):\n",
        "      logits, loss = self.forward(index)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "      next_index = torch.multinomial(probs, num_samples=1, replacement=True)\n",
        "\n",
        "      index = torch.cat((index, next_index), dim=-1)\n",
        "\n",
        "    return index\n",
        "\n",
        "model = GPTLanguageModel(len(chars))\n",
        "m = model.to(device)\n",
        "\n",
        "#index = torch.tensor([1, 2, 3, 4])\n",
        "\n",
        "#logits = model(index, targets=index)\n",
        "#print(logits.shape)"
      ],
      "metadata": {
        "id": "jTcdIZQdUVp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLOCK"
      ],
      "metadata": {
        "id": "r5ba-wJ0sttw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embed, n_head=n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_head\n",
        "    self.sa = MultiHeadAttention(head_size, n_head)\n",
        "    self.ln1 = nn.LayerNorm(n_embed)\n",
        "    self.ln2 = nn.LayerNorm(n_embed)\n",
        "    self.ffwd = FeedFoward(n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y = self.sa(x)\n",
        "    x = self.ln1(x + y)\n",
        "    y = self.ffwd(x)\n",
        "    x = self.ln2(x + y)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "XzCWz9UXssvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEED FORWARD NEURAL NETWORK"
      ],
      "metadata": {
        "id": "GEv-nkyq1Xpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(nn.Linear(n_embed, 4*n_embed), nn.ReLU(), nn.Linear(4*n_embed, n_embed), nn.Dropout(dropout))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.net(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "ZLKKrcw017X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MULTI HEAD ATTENTION"
      ],
      "metadata": {
        "id": "lGoX3oiw6xad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, head_size, n_head):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
        "    self.proj = nn.Linear(n_head * head_size, n_embed)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "    x = self.proj(x)\n",
        "    x = self.dropout(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "QCkG7-WC7uuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HEAD"
      ],
      "metadata": {
        "id": "V9BAniZ-JXNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.querry = nn.Linear(n_embed, head_size)\n",
        "    self.value = nn.Linear(n_embed, head_size)\n",
        "    self.key = nn.Linear(n_embed, head_size)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    q = self.querry(x)\n",
        "    k = self.querry(x)\n",
        "\n",
        "    wei = q @ k.transpose(-2,-1) * k.shape[-1] ** - 0.5\n",
        "    wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "\n",
        "    v = self.value(x)\n",
        "\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "YpBHKaDFJkPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA EXTRACT FILE"
      ],
      "metadata": {
        "id": "eGly_c1gb19q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import lzma\n",
        "from tqdm import tqdm\n",
        "\n",
        "def xz_files_in_dir(directory):\n",
        "  files = []\n",
        "  for filename in os.listdir(directory):\n",
        "    if filename.endswith('xz') and os.path.isfile(os.path.join(directory, filename)):\n",
        "      files.append(filename)\n",
        "\n",
        "  return files\n",
        "\n",
        "\n",
        "folder_path = \"/Users/redfique/Downloads/openwebtext\"\n",
        "output_file = 'output{}.txt'\n",
        "vocab_file = 'vocab.txt'\n",
        "split_files = int(input(\"How many files would you like to split this into?\"))\n",
        "\n",
        "\n",
        "files = xz_files_in_dir(folder_path)\n",
        "\n",
        "total_files = len(files)\n",
        "\n",
        "vocab = set()\n",
        "\n",
        "max_count = total_files // split_files if split_files != 0 else total_files\n",
        "\n",
        "for i in range(split_files):\n",
        "  with open(output_file.format(i), 'w', encoding='utf-8') as outfile:\n",
        "    for count, filename in enumerate(tqdm(files[:max_count], total=max_count)):\n",
        "      if count > max_count:\n",
        "        break\n",
        "\n",
        "      file_path = os.path.join(folder_path, filename)\n",
        "      with lzma.open(file_path, 'rt', encoding=\"utf-8\") as infile:\n",
        "        text = infile.read()\n",
        "        outfile.write(text)\n",
        "        characters = set(text)\n",
        "\n",
        "        vocab.update(characters)\n",
        "\n",
        "    files = files[max_count:]\n",
        "\n",
        "with open(vocab_file, \"w\", encoding='utf-8') as vfile:\n",
        "      for char in vocab:\n",
        "        vfile.write(char)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vpJyE3Odb7VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING AND VAL SPLITS"
      ],
      "metadata": {
        "id": "WslktSqWMgfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import lzma\n",
        "from tqdm import tqdm\n",
        "\n",
        "def xz_files_in_dir(directory):\n",
        "  files = []\n",
        "  for filename in os.listdir(directory):\n",
        "    if filename.endswith('xz') and os.path.isfile(os.path.join(directory, filename)):\n",
        "      files.append(filename)\n",
        "\n",
        "  return files\n",
        "\n",
        "\n",
        "folder_path = \"/Users/redfique/Downloads/openwebtext\"\n",
        "output_train_file = 'output_train.txt'\n",
        "output_val_file = 'output_val.txt'\n",
        "vocab_file = 'vocab.txt'\n",
        "\n",
        "files = xz_files_in_dir(folder_path)\n",
        "\n",
        "total_files = len(files)\n",
        "split_index = int(total_files * 0.9)\n",
        "\n",
        "train_files = files[:split_index]\n",
        "val_files = files[split_index:]\n",
        "\n",
        "vocab = set()\n",
        "\n",
        "with open(output_train_file, 'w', encoding='utf-8') as outfile:\n",
        "  for count, filename in enumerate(train_files, total=len(train_files)):\n",
        "\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "    with lzma.open(file_path, 'rt', encoding=\"utf-8\") as infile:\n",
        "      text = infile.read()\n",
        "      outfile.write(text)\n",
        "      characters = set(text)\n",
        "\n",
        "      vocab.update(characters)\n",
        "\n",
        "with open(output_val_file, 'w', encoding='utf-8') as outfile:\n",
        "  for count, filename in enumerate(val_files, total=len(val_files)):\n",
        "\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "    with lzma.open(file_path, 'rt', encoding=\"utf-8\") as infile:\n",
        "      text = infile.read()\n",
        "      outfile.write(text)\n",
        "      characters = set(text)\n",
        "\n",
        "      vocab.update(characters)\n",
        "\n",
        "with open(vocab_file, \"w\", encoding='utf-8') as vfile:\n",
        "      for char in vocab:\n",
        "        vfile.write(char)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U3hVEEdhMciM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GET BATCH FOR OPENWEBTEXT"
      ],
      "metadata": {
        "id": "XV5FO7qMhnTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mmap\n",
        "import random\n",
        "def get_random_chunk(split):\n",
        "  filename = \"output_train.txt\" if split == \"train\" else \"output_val.txt\"\n",
        "  with open(filename, \"rb\") as f:\n",
        "    with mmap.mmap(f.fileno(), 0, access=mmap.ACCES_READ) as mm:\n",
        "      # file size and random positions\n",
        "      file_size = len(mm)\n",
        "      start_pos = random.randint(0, block_size*batch_size)\n",
        "\n",
        "      # seek to the start position and read the block of text\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lKxK9fKXh4uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VOCABULARIES OF THE OPENWEBTEXT"
      ],
      "metadata": {
        "id": "48MYrVc423v_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('vocab.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "  chars = sorted(list(set(text)))\n",
        "\n",
        "vocab_size = len(chars)\n"
      ],
      "metadata": {
        "id": "Y1hUXvW03LtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOADING MODEL IF EXISTS"
      ],
      "metadata": {
        "id": "7OY70ELA7xF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('loading model parameters .....')\n",
        "with open('model_001.pkl', 'rb') as f:\n",
        "  model = pickle.load(f)\n",
        "model.to(device)\n",
        "print('loaded successfully')\n",
        "\n"
      ],
      "metadata": {
        "id": "qxPmlt6k70ld"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}